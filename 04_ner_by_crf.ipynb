{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF解决NER任务\n",
    "\n",
    "\n",
    "![jupyter](./imgs/crf.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "import sklearn_crfsuite\n",
    "\n",
    "import util\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = './bio/train.bio'\n",
    "test_data_file = './bio/test.bio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(10748, 1343)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取bio格式数据\n",
    "train_sentences = util.read_bio_data(train_data_file)\n",
    "test_sentences = util.read_bio_data(test_data_file)\n",
    "len(train_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[['彭', 'B-nr', 'B-name'],\n ['小', 'I-nr', 'I-name'],\n ['军', 'E-nr', 'I-name'],\n ['认', 'B-v', 'O'],\n ['为', 'E-v', 'O'],\n ['，', 'S-x', 'O'],\n ['国', 'B-s', 'O'],\n ['内', 'E-s', 'O'],\n ['银', 'B-n', 'O'],\n ['行', 'E-n', 'O'],\n ['现', 'B-t', 'O'],\n ['在', 'E-t', 'O'],\n ['走', 'S-v', 'O'],\n ['的', 'S-uj', 'O'],\n ['是', 'S-v', 'O'],\n ['台', 'B-ns', 'B-address'],\n ['湾', 'E-ns', 'I-address'],\n ['的', 'S-uj', 'O'],\n ['发', 'B-n', 'O'],\n ['卡', 'E-n', 'O'],\n ['模', 'B-n', 'O'],\n ['式', 'E-n', 'O'],\n ['，', 'S-x', 'O'],\n ['先', 'S-d', 'O'],\n ['通', 'B-p', 'O'],\n ['过', 'E-p', 'O'],\n ['跑', 'B-n', 'O'],\n ['马', 'E-n', 'O'],\n ['圈', 'B-n', 'O'],\n ['地', 'E-n', 'O']]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构造CRF特征\n",
    "\n",
    "![jupyter](./imgs/crf_features.png)\n",
    "\n",
    "\n",
    "- 转移特征𝑡𝑘(𝑦𝑖−1,𝑦𝑖,𝑥,𝑖)是定义在边上的特征函数（transition），依赖于当前位置i和前一位置i-1，对应的权值为𝜆𝑘 \n",
    "\n",
    "- 状态特征𝑠𝑙(𝑦𝑖,𝑥,𝑖)是定义在节点上的特征函数（state）依赖于当前位置i，对应的权值为𝜇𝑙  \n",
    "\n",
    "- 特征函数的取值为1或0，当满足规定好的特征条件时取值为1，否则为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sent, i):\n",
    "    # 把第i个位置上面的特征拿出来\n",
    "    word = sent[i][0]\n",
    "    pos_tag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        # 是不是字母\n",
    "        'word.isalpha()': word.encode('UTF-8').isalpha(),\n",
    "        # 小写\n",
    "        'word.lower()': word.lower(),\n",
    "        # 大写\n",
    "        'word.isupper()': word.isupper(),\n",
    "        # 数字\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        # pos tag特征如B-v\n",
    "        'pos_tag': pos_tag,\n",
    "        # pos tag边界特征，如B I E S\n",
    "        'pos_tag1': pos_tag.split('-')[0],\n",
    "        # pos tag内容： v/n等\n",
    "        'pos_tag2': pos_tag.split('-')[1],\n",
    "    }\n",
    "    # 如果不是开头，获取前一个位置的所有特征\n",
    "    if i > 0:\n",
    "        # 获取前一个字符\n",
    "        word1 = sent[i - 1][0]\n",
    "        # 获取前一个字符对应的pos tag\n",
    "        pos_tag1 = sent[i - 1][1]\n",
    "        # 将前一个字符的特征加入当前字符的特征\n",
    "        features.update(\n",
    "            {\n",
    "                '-1:word.isalpha()': word1.encode('UTF-8').isalpha(),\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:word.isdigit()': word1.isdigit(),\n",
    "                '-1:pos_tag': pos_tag1,\n",
    "                '-1:pos_tag1': pos_tag1.split('-')[0],\n",
    "                '-1:pos_tag2': pos_tag1.split('-')[1],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        features['BOS'] = True  # 标记是开始beg of seq\n",
    "\n",
    "    if i < len(sent) - 1:  # 如果不是最后一个，获取下一个位置的所有特征\n",
    "        word2 = sent[i + 1][0]\n",
    "        pos_tag2 = sent[i + 1][1]\n",
    "        features.update(\n",
    "            {\n",
    "                '+1:word.isalpha()': word2.encode('UTF-8').isalpha(),\n",
    "                '+1:word.lower()': word2.lower(),\n",
    "                '+1:word.isupper()': word2.isupper(),\n",
    "                '+1:word.isdigit()': word2.isdigit(),\n",
    "                '+1:pos_tag': pos_tag2,\n",
    "                '+1:pos_tag1': pos_tag2.split('-')[0],\n",
    "                '+1:pos_tag2': pos_tag2.split('-')[1],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        features['EOS'] = True  # 标记是结束 end of seq\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent_features(sent):\n",
    "    return [word_features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent_labels(sent):\n",
    "    return [label for token, pos_tag, label in sent]\n",
    "\n",
    "\n",
    "def sent_tokens(sent):\n",
    "    return [token for token, pos_tag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'bias': 1.0,\n 'word.isalpha()': False,\n 'word.lower()': '商',\n 'word.isupper()': False,\n 'word.isdigit()': False,\n 'pos_tag': 'E-nz',\n 'pos_tag1': 'E',\n 'pos_tag2': 'nz',\n '-1:word.isalpha()': False,\n '-1:word.lower()': '浙',\n '-1:word.isupper()': False,\n '-1:word.isdigit()': False,\n '-1:pos_tag': 'B-nz',\n '-1:pos_tag1': 'B',\n '-1:pos_tag2': 'nz',\n '+1:word.isalpha()': False,\n '+1:word.lower()': '银',\n '+1:word.isupper()': False,\n '+1:word.isdigit()': False,\n '+1:pos_tag': 'B-n',\n '+1:pos_tag1': 'B',\n '+1:pos_tag2': 'n'}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征抽取例子\n",
    "sent_features(train_sentences[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对bio数据进行特征抽取，以及标签提取\n",
    "X_train = [sent_features(s) for s in train_sentences]\n",
    "y_train = [sent_labels(s) for s in train_sentences]\n",
    "\n",
    "X_test = [sent_features(s) for s in test_sentences]\n",
    "y_test = [sent_labels(s) for s in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用sklearn-crfsuite进行训练\n",
    "sklearn crfsuite是一个精简的crfsuite（python-crfsuite）包装器，  \n",
    "它提供与scikit-learn兼容的sklearn_crfsuite.CRF estimator。  \n",
    "以使用scikit-learn模型选择实用程序（交叉验证、超参数优化），或者使用joblib保存/加载CRF模型。  \n",
    "https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite\n",
    "\n",
    "#### sklearn_crfsuite.CRF()常用参数  \n",
    "- algorithm (str, optional (default='lbfgs')) ：训练算法。允许值：\n",
    "    + 'lbfgs' - Gradient descent using the L-BFGS method  \n",
    "    + 'l2sgd' - Stochastic Gradient Descent with L2 regularization term  \n",
    "    + 'ap' - Averaged Perceptron  \n",
    "    + 'pa' - Passive Aggressive (PA)   \n",
    "    + 'arow' - Adaptive Regularization Of Weight Vector (AROW)  \n",
    "- min_freq (float, optional (default=0)) ：特征出现频率的截止阈值。CRFsuite将忽略训练数据中出现频率不大于min\\u freq的特征。默认值为no cut-off。同CRF++中的-f参数。\n",
    "- all_possible_states (bool, optional (default=False)) ：指定 CRFsuite 是否生成在训练数据中甚至不出现的状态特征（即，负状态特征）。当为 True 时，CRFsuite 生成状态特征，这些特征将属性和标签之间的所有可能组合相关联。假设属性和标签的数量分别为 A 和 L，该函数将生成 (A * L) 个特征。 启用此功能可能会提高标记准确性，因为 CRF 模型可以学习到一个item预测的label与参考label不一致的情况。 但是，此功能也可能会增加特征数量并大大减慢训练过程。 默认情况下禁用此功能。\n",
    "- all_possible_transitions (bool, optional (default=False)) ：指定CRFsuite是否生成训练数据中甚至没有出现的转移特征（即负转移特征）。如果为True，CRFsuite将生成关联所有可能标签对的转移特征，f(s', s, o=null)，其中s为t时刻的的标签（y），o为t时刻的上下文（x）。假设训练数据中的标签个数为L，该函数将生成（L*L）个转移特征。默认情况下禁用此功能。\n",
    "- c1 (float, optional (default=0)) ：L1 正则化的系数。如果指定了非零值，CRFsuite 将切换到 Orthant-Wise Limited-memory Quasi-Newton (OWL-QN，Orthant-Wise 有限记忆拟牛顿) 方法。默认值为零（无 L1 正则化）。支持的训练算法：lbfgs。\n",
    "- c2 (float, optional (default=1.0))：L2 正则化的系数。 支持的训练算法：l2sgd、lbfgs。\n",
    "- max_iterations (int, optional (default=None)) ：优化算法的最大迭代次数。默认值取决于训练算法：\n",
    "    + lbfgs - unlimited;  \n",
    "    + l2sgd - 1000;   \n",
    "    + ap - 100;  \n",
    "    + pa - 100;  \n",
    "    + arow - 100.  \n",
    "- num_memories (int, optional (default=6)) ：用于逼近逆 Hessian 矩阵的有限内存数量。 支持的训练算法：lbfgs。\n",
    "- epsilon (float, optional (default=1e-5))：确定收敛条件的 epsilon 参数。 支持的训练算法：ap、arow、lbfgs、pa\n",
    "- period (int, optional (default=10)) ：测试停止标准的迭代持续时间。 支持的训练算法：l2sgd、lbfgs\n",
    "- delta (float, optional (default=1e-5)) ：停止标准的阈值；当最后一个周期迭代中对数似然的改进不大于此阈值时，迭代停止。 支持的训练算法：l2sgd、lbfgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',  # 训练算法\n",
    "    c1=0.0,  # l1正则\n",
    "    c2=1.0,  # l2正则\n",
    "    max_iterations=500,  # 最大迭代次数\n",
    "    all_possible_transitions=True,  # 生成关联所有可能标签对的转移特征\n",
    "    verbose=True  # 过程可视化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 10748/10748 [00:04<00:00, 2398.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 49975\n",
      "Seconds required: 1.742\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 1.000000\n",
      "num_memories: 6\n",
      "max_iterations: 500\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=2.19  loss=695418.84 active=49975 feature_norm=1.00\n",
      "Iter 2   time=2.03  loss=439787.83 active=49975 feature_norm=2.98\n",
      "Iter 3   time=1.05  loss=387484.54 active=49975 feature_norm=2.59\n",
      "Iter 4   time=2.10  loss=356803.40 active=49975 feature_norm=2.29\n",
      "Iter 5   time=1.09  loss=345023.91 active=49975 feature_norm=2.32\n",
      "Iter 6   time=1.06  loss=319775.35 active=49975 feature_norm=2.95\n",
      "Iter 7   time=1.09  loss=291709.40 active=49975 feature_norm=3.85\n",
      "Iter 8   time=1.09  loss=258508.09 active=49975 feature_norm=5.31\n",
      "Iter 9   time=1.04  loss=231355.93 active=49975 feature_norm=7.25\n",
      "Iter 10  time=1.15  loss=200224.66 active=49975 feature_norm=10.19\n",
      "Iter 11  time=1.09  loss=183569.03 active=49975 feature_norm=12.70\n",
      "Iter 12  time=1.10  loss=178751.79 active=49975 feature_norm=13.11\n",
      "Iter 13  time=1.03  loss=169636.70 active=49975 feature_norm=13.66\n",
      "Iter 14  time=1.07  loss=159153.63 active=49975 feature_norm=14.85\n",
      "Iter 15  time=1.07  loss=152427.28 active=49975 feature_norm=14.81\n",
      "Iter 16  time=1.03  loss=143066.62 active=49975 feature_norm=15.38\n",
      "Iter 17  time=1.08  loss=138472.55 active=49975 feature_norm=16.22\n",
      "Iter 18  time=1.02  loss=134162.27 active=49975 feature_norm=16.72\n",
      "Iter 19  time=1.08  loss=128022.17 active=49975 feature_norm=17.88\n",
      "Iter 20  time=1.08  loss=121259.49 active=49975 feature_norm=18.61\n",
      "Iter 21  time=1.03  loss=117350.95 active=49975 feature_norm=18.72\n",
      "Iter 22  time=1.05  loss=111332.40 active=49975 feature_norm=18.06\n",
      "Iter 23  time=1.21  loss=109799.67 active=49975 feature_norm=18.35\n",
      "Iter 24  time=1.17  loss=106056.90 active=49975 feature_norm=19.26\n",
      "Iter 25  time=1.15  loss=99926.66 active=49975 feature_norm=21.40\n",
      "Iter 26  time=1.10  loss=96363.32 active=49975 feature_norm=22.09\n",
      "Iter 27  time=1.10  loss=92916.22 active=49975 feature_norm=22.50\n",
      "Iter 28  time=1.17  loss=89572.86 active=49975 feature_norm=23.26\n",
      "Iter 29  time=1.26  loss=87364.93 active=49975 feature_norm=23.16\n",
      "Iter 30  time=1.17  loss=85382.34 active=49975 feature_norm=23.18\n",
      "Iter 31  time=1.22  loss=83800.62 active=49975 feature_norm=23.56\n",
      "Iter 32  time=1.24  loss=82398.56 active=49975 feature_norm=23.82\n",
      "Iter 33  time=1.19  loss=80579.79 active=49975 feature_norm=24.49\n",
      "Iter 34  time=1.24  loss=79377.13 active=49975 feature_norm=25.34\n",
      "Iter 35  time=1.16  loss=78147.47 active=49975 feature_norm=25.69\n",
      "Iter 36  time=1.22  loss=76696.38 active=49975 feature_norm=26.29\n",
      "Iter 37  time=1.24  loss=75009.48 active=49975 feature_norm=27.12\n",
      "Iter 38  time=1.17  loss=73390.84 active=49975 feature_norm=28.43\n",
      "Iter 39  time=1.15  loss=71718.18 active=49975 feature_norm=29.12\n",
      "Iter 40  time=1.09  loss=70592.36 active=49975 feature_norm=29.59\n",
      "Iter 41  time=1.10  loss=69737.15 active=49975 feature_norm=29.71\n",
      "Iter 42  time=1.03  loss=68673.71 active=49975 feature_norm=30.68\n",
      "Iter 43  time=1.18  loss=67850.43 active=49975 feature_norm=31.30\n",
      "Iter 44  time=1.25  loss=67329.60 active=49975 feature_norm=31.59\n",
      "Iter 45  time=1.19  loss=66060.81 active=49975 feature_norm=32.41\n",
      "Iter 46  time=1.03  loss=64793.98 active=49975 feature_norm=33.50\n",
      "Iter 47  time=1.05  loss=63783.18 active=49975 feature_norm=34.35\n",
      "Iter 48  time=1.18  loss=62897.11 active=49975 feature_norm=35.01\n",
      "Iter 49  time=1.12  loss=62320.16 active=49975 feature_norm=34.95\n",
      "Iter 50  time=1.14  loss=61494.01 active=49975 feature_norm=34.89\n",
      "Iter 51  time=1.11  loss=61188.95 active=49975 feature_norm=34.96\n",
      "Iter 52  time=1.13  loss=60480.91 active=49975 feature_norm=35.05\n",
      "Iter 53  time=1.10  loss=60311.03 active=49975 feature_norm=35.03\n",
      "Iter 54  time=1.04  loss=59810.10 active=49975 feature_norm=35.13\n",
      "Iter 55  time=1.09  loss=59123.79 active=49975 feature_norm=35.38\n",
      "Iter 56  time=1.15  loss=58546.17 active=49975 feature_norm=35.68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/96/0jnny1gj02j8pgfvb9by368m0000gn/T/ipykernel_93881/1310547495.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# 在训练集上训练\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mcrf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/anaconda3/envs/tele/lib/python3.8/site-packages/sklearn_crfsuite/estimator.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, X_dev, y_dev)\u001B[0m\n\u001B[1;32m    329\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 331\u001B[0;31m         \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodelfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mholdout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mX_dev\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    332\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_log_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogparser\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpycrfsuite/_pycrfsuite.pyx\u001B[0m in \u001B[0;36mpycrfsuite._pycrfsuite.BaseTrainer.train\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpycrfsuite/_pycrfsuite.pyx\u001B[0m in \u001B[0;36mpycrfsuite._pycrfsuite.BaseTrainer._on_message\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpycrfsuite/_pycrfsuite.pyx\u001B[0m in \u001B[0;36mpycrfsuite._pycrfsuite.Trainer.message\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/usr/local/anaconda3/envs/tele/lib/python3.8/site-packages/pycrfsuite/_logparser.py\u001B[0m in \u001B[0;36mfeed\u001B[0;34m(self, line)\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevents\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m     \u001B[0;32mdef\u001B[0m \u001B[0mfeed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m         \u001B[0;31m# if line != '\\n':\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 在训练集上训练\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['crf/crf.model']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 保存模型\n",
    "model_path = 'crf/crf.model'\n",
    "joblib.dump(crf, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['B-company',\n 'I-company',\n 'B-name',\n 'I-name',\n 'B-game',\n 'I-game',\n 'B-organization',\n 'I-organization',\n 'B-movie',\n 'I-movie',\n 'B-position',\n 'I-position',\n 'B-address',\n 'I-address',\n 'B-government',\n 'I-government',\n 'B-scene',\n 'I-scene',\n 'B-book',\n 'I-book']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取所有labels\n",
    "labels = list(crf.classes_)\n",
    "# 去掉'O'标签，只关注实体的标签\n",
    "labels.remove('O')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模型进行预测\n",
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "B-organization       0.52      0.39      0.44       206\n",
      "     B-company       0.64      0.53      0.58       279\n",
      "  B-government       0.73      0.60      0.66       190\n",
      "        I-name       0.59      0.54      0.56       732\n",
      "    B-position       0.69      0.53      0.60       347\n",
      "        B-game       0.67      0.71      0.69       226\n",
      "       B-scene       0.56      0.31      0.40       124\n",
      "       I-scene       0.64      0.41      0.50       458\n",
      "       I-movie       0.61      0.56      0.59       580\n",
      "        I-book       0.67      0.54      0.60       715\n",
      "     B-address       0.57      0.38      0.46       273\n",
      "        I-game       0.69      0.74      0.71      1065\n",
      "        B-name       0.67      0.56      0.61       352\n",
      "       B-movie       0.58      0.47      0.52       101\n",
      "     I-company       0.63      0.56      0.59      1031\n",
      "  I-government       0.68      0.58      0.63       855\n",
      "    I-position       0.67      0.54      0.60       610\n",
      "        B-book       0.64      0.54      0.59       121\n",
      "I-organization       0.52      0.38      0.44       688\n",
      "     I-address       0.64      0.55      0.59      1045\n",
      "\n",
      "     micro avg       0.64      0.55      0.59      9998\n",
      "     macro avg       0.63      0.52      0.57      9998\n",
      "  weighted avg       0.64      0.55      0.59      9998\n",
      "\n",
      "evaluate result: \n",
      "Tag\tPrecision\tRecall\tF1\n",
      "     movie\t58.02\t46.53\t51.65\n",
      "      book\t63.37\t52.89\t57.66\n",
      "      game\t65.13\t68.58\t66.81\n",
      "   address\t46.45\t31.14\t37.28\n",
      "organization\t49.35\t36.89\t42.22\n",
      "     scene\t52.86\t29.84\t38.14\n",
      "   company\t59.21\t48.39\t53.25\n",
      "government\t67.52\t55.79\t61.10\n",
      "  position\t68.03\t52.74\t59.42\n",
      "      name\t66.10\t54.83\t59.94\n",
      "     total\t60.97\t48.72\t54.16\n"
     ]
    }
   ],
   "source": [
    "# 将真实标签列表与预测标签列表展开成一个大的列表，进行结果评价\n",
    "true_bio = [x for y in y_test for x in y]\n",
    "predict_bio = [x for y in y_pred for x in y]\n",
    "\n",
    "# 输出评价结果\n",
    "metric_result = util.measure_by_tags(true_bio, predict_bio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.0, c2=1.0,\n    keep_tempfiles=None, max_iterations=500, verbose=True)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# 加载模型\n",
    "\n",
    "model_path = 'crf/crf.model'\n",
    "saved_model = joblib.load(model_path)\n",
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CRF模型的批量预测pipeline，pos tag特征抽取 -> CRF特征函数抽取 -> 模型预测 \n",
    "def predict_pipeline(model, sentences):\n",
    "    features = []\n",
    "    for sentence in sentences:\n",
    "        posseg_list = util.get_word_posseg_feature(sentence)  # 提取pos tag BIOES特征\n",
    "        sentence_with_posseg = [(sentence[index], posseg_list[index]) for index in range(len(sentence))]\n",
    "        features.append(sent_features(sentence_with_posseg))  # 提取CRF特征\n",
    "    y_predicts = model.predict(features)  # 使用模型做批量预测\n",
    "    return y_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = ['李正茂出任中国电信集团有限公司总经理。',\n",
    "                  '2012年成立中国电信国际有限公司,总部设于中国香港。',\n",
    "                  '《长津湖》将于今年下半年上映。']\n",
    "\n",
    "# 批量预测bio序列结果\n",
    "y_predicts = predict_pipeline(saved_model, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李正茂出任中国电信集团有限公司总经理。\n",
      "['B-name', 'I-name', 'I-name', 'O', 'O', 'B-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'B-position', 'I-position', 'I-position', 'O']\n",
      "[\n",
      "    {\n",
      "        \"begin\": 0,\n",
      "        \"end\": 3,\n",
      "        \"tag\": \"name\",\n",
      "        \"word\": \"李正茂\"\n",
      "    },\n",
      "    {\n",
      "        \"begin\": 5,\n",
      "        \"end\": 15,\n",
      "        \"tag\": \"company\",\n",
      "        \"word\": \"中国电信集团有限公司\"\n",
      "    },\n",
      "    {\n",
      "        \"begin\": 15,\n",
      "        \"end\": 18,\n",
      "        \"tag\": \"position\",\n",
      "        \"word\": \"总经理\"\n",
      "    }\n",
      "]\n",
      "2012年成立中国电信国际有限公司,总部设于中国香港。\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'O', 'O', 'O', 'O', 'O', 'B-address', 'I-address', 'I-address', 'I-address', 'O']\n",
      "[\n",
      "    {\n",
      "        \"begin\": 7,\n",
      "        \"end\": 17,\n",
      "        \"tag\": \"company\",\n",
      "        \"word\": \"中国电信国际有限公司\"\n",
      "    },\n",
      "    {\n",
      "        \"begin\": 22,\n",
      "        \"end\": 26,\n",
      "        \"tag\": \"address\",\n",
      "        \"word\": \"中国香港\"\n",
      "    }\n",
      "]\n",
      "《长津湖》将于今年下半年上映。\n",
      "['B-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[\n",
      "    {\n",
      "        \"begin\": 0,\n",
      "        \"end\": 5,\n",
      "        \"tag\": \"movie\",\n",
      "        \"word\": \"《长津湖》\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for pred, sentence in zip(y_predicts, test_sentences):\n",
    "    print(sentence)\n",
    "    print(pred)\n",
    "    # 将bio序列转化为实体序列\n",
    "    entitys = util.bio_2_entities(pred)\n",
    "    # 格式化展示实体序列结果\n",
    "    result = util.formatting_result(entitys, sentence)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-b35e09b3",
   "language": "python",
   "display_name": "PyCharm (Chapter.4)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}