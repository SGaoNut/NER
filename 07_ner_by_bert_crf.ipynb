{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT + BiLSTM + CRF\n",
    "\n",
    "#### 将前面的BiLSTM + CRF与BERT + Softmax结合起来\n",
    "\n",
    "![jupyter](./imgs/bert_bilstm_crf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.699 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "import util\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 构造数据集\n",
    "\n",
    "#### 与前面BERT+softmax做NER处理方式相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sample_to_feature(text, max_length):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def map_sample_to_dict(input_ids, token_type_ids, attention_masks, label):\n",
    "    return {\n",
    "               \"input_ids\": input_ids,\n",
    "               \"token_type_ids\": token_type_ids,\n",
    "               \"attention_mask\": attention_masks,\n",
    "           }, label\n",
    "\n",
    "\n",
    "def build_dataset(samples, tag_2_id, max_length, batch_size, is_train):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for sample in samples:\n",
    "        text = [x[0] for x in sample]\n",
    "        label = [tag_2_id.get(x[2], 0) for x in sample][: max_length - 1]\n",
    "        # 开头加PAD，即CLS\n",
    "        label.insert(0, 0)\n",
    "        bert_input = convert_sample_to_feature(text, max_length)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append(label)\n",
    "    label_list = pad_sequences(label_list, padding='post', maxlen=max_length, )\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (input_ids_list, attention_mask_list, token_type_ids_list, label_list)\n",
    "    )\n",
    "    dataset = dataset.map(map_sample_to_dict)\n",
    "    buffer_size = len(label_list)\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_path = './data/dataset.pkl'\n",
    "\n",
    "with open(dataset_save_path, 'rb') as f:\n",
    "    train_sentences, val_sentences, test_sentences, tag_2_id, id_2_tag = pickle.load(f)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_SEQ_LEN = 52\n",
    "\n",
    "# dataset\n",
    "train_dataset = build_dataset(train_sentences, tag_2_id, MAX_SEQ_LEN, BATCH_SIZE, True)\n",
    "val_dataset = build_dataset(val_sentences, tag_2_id, MAX_SEQ_LEN, BATCH_SIZE, False)\n",
    "test_dataset = build_dataset(test_sentences, tag_2_id, MAX_SEQ_LEN, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tf_ad\n",
    "\n",
    "\n",
    "class NerModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_dim, label_size, dropout_rate=0.5):\n",
    "        super(NerModel, self).__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.label_size = label_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # 使用中文BERT base模型\n",
    "        self.bert = TFBertModel.from_pretrained('bert-base-chinese')\n",
    "        # BiLSTM层\n",
    "        self.biLSTM = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                self.lstm_dim,\n",
    "                return_sequences=True,\n",
    "                activation='tanh',\n",
    "                activity_regularizer='l2',\n",
    "                dropout=self.dropout_rate\n",
    "            )\n",
    "        )\n",
    "        # 标签分类层，提取发射分数\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            self.label_size, activation='relu', activity_regularizer='l2'\n",
    "        )\n",
    "        # 定义CRF转移矩阵，提取转移分数\n",
    "        self.transition_params = tf.Variable(\n",
    "            tf.random.uniform(shape=(self.label_size, self.label_size))\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, labels=None, training=None):\n",
    "        # 获取原始文本的真实长度，即token id不为0的长度\n",
    "        text_lens = tf.math.reduce_sum(tf.cast(tf.math.not_equal(inputs['input_ids'], 0), dtype=tf.int32), axis=-1)\n",
    "        # 取出BERT另一种输出last_hidden_state，然后特征抽取器\n",
    "        # 不是只取第一个，是把所有的hidden_state都取出来\n",
    "        X = self.bert(inputs)[0]\n",
    "        X = self.biLSTM(X)  # bilstm特征抽取\n",
    "        logits = self.dense(X)  # 发射分数\n",
    "        # 如果label不为空，可以算loss\n",
    "        if labels is not None:\n",
    "            # 将标签序列转化为tf tensor\n",
    "            label_sequences = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "            # 使用tf_ad.text.crf_log_likelihood定义crf层，获取crf loss以及更新转移矩阵\n",
    "            log_likelihood, self.transition_params = tf_ad.text.crf_log_likelihood(\n",
    "                logits,\n",
    "                label_sequences,\n",
    "                text_lens,\n",
    "                transition_params=self.transition_params)\n",
    "            # 返回发射分数，文本真实长度，crf loss\n",
    "            return logits, text_lens, log_likelihood\n",
    "        else:\n",
    "            # 返回发射分数，文本真实长度\n",
    "            return logits, text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "LSTM_DIM = 256\n",
    "LR = 3e-5\n",
    "DROPOUT = 0.5\n",
    "label_size = len(tag_2_id)\n",
    "\n",
    "# 定义BERT + BiLSTM + CRF模型\n",
    "model = NerModel(LSTM_DIM, label_size, DROPOUT)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './bert_crf'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(output_dir))\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt,\n",
    "    output_dir,\n",
    "    checkpoint_name='model.ckpt',\n",
    "    max_to_keep=1  # bert模型较大，这里只保存1个\n",
    ")\n",
    "\n",
    "\n",
    "# 定义一次batch计算过程\n",
    "def run_one_step(model, text_batch, labels_batch, training=True):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 取出模型前向运算的发射分数、文本真实长度、crf loss\n",
    "        logits, text_lens, log_likelihood = model(text_batch, labels_batch, training)\n",
    "        # 将batch的crf loss进行平均\n",
    "        loss = - tf.reduce_mean(log_likelihood)\n",
    "    if training:\n",
    "        # 如果是训练，需要通过优化器进行梯度的更新\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients((grad, var)\n",
    "                                  for (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                                  if grad is not None)  # 验证、测试阶段无需更新梯度\n",
    "    return loss, logits, text_lens\n",
    "\n",
    "\n",
    "# 定义模型预测\n",
    "def predict_result(model, dataset, id_2_tag):\n",
    "    # 初始化loss、预测标签、真实标签列表\n",
    "    losses, preds, trues = [], [], []\n",
    "    # 对dataset进行batch计算\n",
    "    for _, (text_batch, labels_batch) in enumerate(dataset):\n",
    "        # 进行一次前向计算，获取crf loss、发射分数、文本真实长度\n",
    "        loss, logits, text_lens = run_one_step(model, text_batch, labels_batch, False)\n",
    "        losses.append(loss)\n",
    "        for logit, text_len, labels in zip(logits, text_lens, labels_batch):\n",
    "            # 根据序列真实长度使用维特比解码出最优序列\n",
    "            viterbi_path, _ = tf_ad.text.viterbi_decode(logit[:text_len], model.transition_params)\n",
    "            # 将最优序列作为预测序列\n",
    "            preds.extend(viterbi_path)\n",
    "            # 还原真实的标签序列\n",
    "            trues.extend(labels.numpy()[: text_len])\n",
    "    # 将标签id还原为标签\n",
    "    true_bios = [id_2_tag[i] for i in trues]\n",
    "    predict_bios = [id_2_tag[i] for i in preds]\n",
    "    return true_bios, predict_bios, losses\n",
    "\n",
    "\n",
    "# 结果评价，主要用于训练过程中查看验证集结果\n",
    "def metrics(model, dataset, tags):\n",
    "    true_bios, predict_bios, losses = predict_result(model, dataset, tags)\n",
    "    f1_score = util.get_f1_score(true_bios, predict_bios)  # 基于实体的f1 score\n",
    "    avg_loss = sum(losses) / len(losses)  # 平均的loss\n",
    "    return f1_score, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 100, train_loss 3.2672343254089355\n",
      "Epoch 0, step 200, train_loss 2.183166980743408\n",
      "Epoch 0, step 300, train_loss 2.5725584030151367\n",
      "Epoch 0, step 400, train_loss 4.9358391761779785\n",
      "Epoch 0, step 500, train_loss 1.9492483139038086\n",
      "Epoch 0, step 600, train_loss 2.7126665115356445\n",
      "Epoch 1, step 700, train_loss 1.1260156631469727\n",
      "Epoch 1, step 800, train_loss 4.498970985412598\n",
      "Epoch 1, step 900, train_loss 4.033175468444824\n",
      "Epoch 1, step 1000, train_loss 2.9692764282226562\n",
      "Epoch 1, step 1100, train_loss 2.8545150756835938\n",
      "Epoch 1, step 1200, train_loss 3.945244789123535\n",
      "Epoch 2, step 1300, train_loss 0.8673095703125\n",
      "Validation Result: val_f1 0.7199657973492946, val_loss 13.195147514343262\n",
      "New best f1: 0.7199657973492946, model saved!\n",
      "Epoch 2, step 1400, train_loss 1.7509288787841797\n",
      "Validation Result: val_f1 0.719929762949956, val_loss 12.84805679321289\n",
      "Epoch 2, step 1500, train_loss 2.1297292709350586\n",
      "Validation Result: val_f1 0.7131252672082086, val_loss 13.332390785217285\n",
      "Epoch 2, step 1600, train_loss 1.4410324096679688\n",
      "Validation Result: val_f1 0.7062847370671226, val_loss 12.796889305114746\n",
      "Epoch 2, step 1700, train_loss 0.7826204299926758\n",
      "Validation Result: val_f1 0.7085665088247954, val_loss 13.163223266601562\n",
      "Epoch 2, step 1800, train_loss 1.7636785507202148\n",
      "Validation Result: val_f1 0.702303346371143, val_loss 12.484817504882812\n",
      "Epoch 3, step 1900, train_loss 2.0309953689575195\n",
      "Validation Result: val_f1 0.7147147147147146, val_loss 13.182592391967773\n",
      "Early stoped!\n",
      "Train finished\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10  # 迭代次数\n",
    "best_f1 = 0.0  # 记录最优的f1 score\n",
    "step = 0  # 记录训练步数\n",
    "early_stop_step = 0  # 记录早停步数\n",
    "STOP_STEP = 5  # 设置早停等待步数\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (text_batch, labels_batch) in train_dataset:\n",
    "        step = step + 1\n",
    "        # 一次训练过程，只取出loss\n",
    "        loss, _, _ = run_one_step(model, text_batch, labels_batch, True)\n",
    "        if step % 100 == 0:  # 每隔100步打印训练的中间结果\n",
    "            print(f'Epoch {epoch}, step {step}, train_loss {loss}')\n",
    "            if epoch > 1:  # 从第2个epoch开始计算验证集结果\n",
    "                # 计算验证集的实体分类f1 score，以及loss\n",
    "                f1_score, avg_loss = metrics(model, val_dataset, id_2_tag)\n",
    "                print(f'Validation Result: val_f1 {f1_score}, val_loss {avg_loss}')\n",
    "                # 记录最优的f1 score\n",
    "                if f1_score > best_f1:\n",
    "                    best_f1 = f1_score\n",
    "                    ckpt_manager.save()  # 记录最优时模型的权重\n",
    "                    print(f'New best f1: {best_f1}, model saved!')\n",
    "                    early_stop_step = 0\n",
    "                else:\n",
    "                    early_stop_step += 1\n",
    "                # 连续一定步数最优f1不再变化，则早停\n",
    "                if early_stop_step > STOP_STEP:\n",
    "                    print('Early stoped!')\n",
    "                    break\n",
    "    if early_stop_step > STOP_STEP:\n",
    "        break\n",
    "\n",
    "print(\"Train finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ner_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tf_bert_model (TFBertModel)  multiple                  102267648 \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  10773     \n",
      "=================================================================\n",
      "Total params: 104,378,062\n",
      "Trainable params: 104,378,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 查看模型结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "  B-government       0.76      0.81      0.79       190\n",
      "        B-book       0.77      0.83      0.80       121\n",
      "       B-scene       0.66      0.60      0.63       124\n",
      "       I-movie       0.87      0.81      0.84       580\n",
      "     I-company       0.73      0.82      0.77      1031\n",
      "        B-name       0.83      0.91      0.87       352\n",
      "I-organization       0.65      0.64      0.64       688\n",
      "  I-government       0.79      0.83      0.81       855\n",
      "     B-address       0.62      0.73      0.67       273\n",
      "        B-game       0.81      0.89      0.85       226\n",
      "    I-position       0.73      0.79      0.76       610\n",
      "     I-address       0.69      0.80      0.74      1045\n",
      "        I-book       0.84      0.89      0.87       715\n",
      "    B-position       0.72      0.78      0.75       347\n",
      "        I-name       0.84      0.89      0.86       732\n",
      "     B-company       0.76      0.84      0.79       279\n",
      "        I-game       0.82      0.90      0.86      1065\n",
      "       I-scene       0.69      0.59      0.64       458\n",
      "       B-movie       0.84      0.75      0.79       101\n",
      "B-organization       0.65      0.67      0.66       206\n",
      "\n",
      "     micro avg       0.76      0.81      0.78      9998\n",
      "     macro avg       0.75      0.79      0.77      9998\n",
      "  weighted avg       0.76      0.81      0.78      9998\n",
      "\n",
      "evaluate result: \n",
      "Tag\tPrecision\tRecall\tF1\n",
      "   company\t69.48\t76.70\t72.91\n",
      "     scene\t59.65\t54.84\t57.14\n",
      "  position\t70.82\t76.95\t73.76\n",
      "   address\t52.94\t62.64\t57.38\n",
      "      name\t80.00\t87.50\t83.58\n",
      "government\t72.28\t76.84\t74.49\n",
      "     movie\t75.82\t68.32\t71.87\n",
      "      book\t73.28\t79.34\t76.19\n",
      "organization\t61.32\t63.11\t62.20\n",
      "      game\t75.81\t83.19\t79.32\n",
      "     total\t69.30\t74.67\t71.89\n"
     ]
    }
   ],
   "source": [
    "# 使用训练集进行模型评估\n",
    "true_bios, predict_bios, _ = predict_result(model, test_dataset, id_2_tag)\n",
    "metric_result = util.measure_by_tags(true_bios, predict_bios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4590781d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型进行预测\n",
    "output_dir = './bert_crf'\n",
    "saved_model = NerModel(\n",
    "    LSTM_DIM,\n",
    "    label_size,\n",
    "    DROPOUT)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "\n",
    "# 从Checkpoint中还原模型权重\n",
    "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=saved_model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在线预测\n",
    "predict_sentences = [\n",
    "    '李正茂出任中国电信集团有限公司总经理。',\n",
    "    '2012年成立中国电信国际有限公司,总部设于中国香港。',\n",
    "    '《长津湖》将于今年下半年上映。'\n",
    "]\n",
    "\n",
    "\n",
    "def build_predict_sampe(sentence):\n",
    "    return [(word, _, 'O') for word in sentence]\n",
    "\n",
    "\n",
    "predict_samples = [build_predict_sampe(sentence) for sentence in predict_sentences]\n",
    "predict_dataset = build_dataset(predict_samples, tag_2_id, MAX_SEQ_LEN, 3, False)\n",
    "\n",
    "# 使用模型进行预测\n",
    "logits, text_lens = saved_model.predict(predict_dataset)\n",
    "paths = []\n",
    "for logit, text_len in zip(logits, text_lens):\n",
    "    # 维特比解码出最优序列\n",
    "    viterbi_path, _ = tf_ad.text.viterbi_decode(logit[1: text_len + 1], saved_model.transition_params)\n",
    "    paths.append(viterbi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李正茂出任中国电信集团有限公司总经理。\n",
      "['B-name', 'I-name', 'I-name', 'O', 'O', 'B-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'B-position', 'I-position', 'I-position', 'O', 'O', 'O']\n",
      "[\n",
      "    {\n",
      "        \"begin\": 0,\n",
      "        \"end\": 3,\n",
      "        \"tag\": \"name\",\n",
      "        \"word\": \"李正茂\"\n",
      "    },\n",
      "    {\n",
      "        \"begin\": 5,\n",
      "        \"end\": 15,\n",
      "        \"tag\": \"company\",\n",
      "        \"word\": \"中国电信集团有限公司\"\n",
      "    },\n",
      "    {\n",
      "        \"begin\": 15,\n",
      "        \"end\": 18,\n",
      "        \"tag\": \"position\",\n",
      "        \"word\": \"总经理\"\n",
      "    }\n",
      "]\n",
      "2012年成立中国电信国际有限公司,总部设于中国香港。\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'I-company', 'O', 'O', 'O', 'O', 'O', 'B-address', 'I-address', 'I-address', 'I-address', 'O', 'O', 'O']\n",
      "[\n",
      "    {\n",
      "        \"begin\": 7,\n",
      "        \"end\": 17,\n",
      "        \"tag\": \"company\",\n",
      "        \"word\": \"中国电信国际有限公司\"\n",
      "    },\n",
      "    {\n",
      "        \"begin\": 22,\n",
      "        \"end\": 26,\n",
      "        \"tag\": \"address\",\n",
      "        \"word\": \"中国香港\"\n",
      "    }\n",
      "]\n",
      "《长津湖》将于今年下半年上映。\n",
      "['B-movie', 'I-movie', 'I-movie', 'I-movie', 'I-movie', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[\n",
      "    {\n",
      "        \"begin\": 0,\n",
      "        \"end\": 5,\n",
      "        \"tag\": \"movie\",\n",
      "        \"word\": \"《长津湖》\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 结果展示\n",
    "for text, path in zip(test_sentences, paths):\n",
    "    print(text)\n",
    "    bio_seq = [id_2_tag[tag_id] for tag_id in path]\n",
    "    print(bio_seq)\n",
    "    entities_result = util.bio_2_entities(bio_seq)\n",
    "    json_result = util.formatting_result(entities_result, text)\n",
    "    print(json_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}